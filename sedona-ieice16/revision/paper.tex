\documentclass[letter]{ieice}
\usepackage{balance}
\usepackage{flushend}
\usepackage[ruled,linesnumbered]{algorithm2e}
\usepackage{amssymb,amsthm,amsmath}
\usepackage{balance}
\usepackage{graphicx}          % graphicx package for including ps files  
\usepackage{epsfig}
\usepackage{url}
\usepackage{subfigure}
\usepackage{algorithmic}
\usepackage{float}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\usepackage{latexsym}
\usepackage{multirow}
\usepackage{color}
\usepackage{url}
\usepackage{comment}
%\usepackage{pstricks}
\usepackage{array}
\usepackage{epstopdf}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\setcounter{page}{1}
%\breakauthorline{}% breaks lines after the n-th author

\field{D}
\title{SEDONA: A Novel Protocol for Identifying Infrequent, Long-Running Daemons on a Linux System}
%\titlenote{This work was in part supported by the EDISON program (NRF-2011-0020576).}
\authorlist{
 \authorentry[yksuh@kisti.re.kr]{Young-Kyoon SUH}{m}{labelA}
 %\authorentry[rts@cs.arizona.edu]{Richard T. Snodgrass}{n}{labelB}}
\affiliate[labelA]{Supercomputing R\&{D} Center, KISTI, 245 Daehangno, Yuseong-gu, Daejeon, 34141, Republic of Korea}
%\affiliate[labelB]{Department of Computer Science, University of Arizona, Tucson, AZ 85721, USA}
}
\received{2017}{2}{10}

\definecolor{grey}{RGB}{200,200,200}
\newcommand{\hilite}[1]{\colorbox{grey}{#1}}
\newcommand{\hilitey}[1]{\colorbox{yellow}{#1}}
\newcommand{\hiliting}[1]{\colorbox{grey}{#1}}
\long\def\todo#1{\hilitey{{\bf TODO:} {\em #1}}}
\long\def\shorten#1{}
\long\def\comment#1{}

\begin{document}

\maketitle

\begin{summary}
{\color{blue}Measuring program execution time 
a \hbox{much-used} technique for performance evaluation in computer science. 
Without a proper care, however, timed results may vary a lot, 
thus making it hard to trust their validity. 
In this paper we propose a novel timing protocol to significantly 
reduce such variability.}

\end{summary}
\begin{keywords}
Infrequent Long-running Daemon, Execution-Time Measurement
\end{keywords}

\section{Introduction}
\label{sec:intro}

Measuring program execution time is a much-used
technique for performance evaluation in computer science. 
Despite the importance of accurate and precise execution-time measurement, 
how to achieve {\em better} timing has not been well addressed. 
Surprisingly, there is considerable variability in the measured time.
%which indeed suggests the need for a better timing protocol.
{\color{red} The goal of this paper is to propose
a better timing protocol that can significantly reduce such variability 
and thus enable better timing results without a doubt of their validity.}

Consider a simple compute-bound program, what we term {\em PUT} (program-under-test), 
as shown in Fig.~\ref{alg:put}. 
This program runs a nested for-loop with a specified task length ($tl$) (in seconds). 
{\color{blue}The $tl$ value is used to compute 
the number of iterations ($t$) for which that for-loop is performed 
to reach the specified task length.}

\vspace{-.2in}
\begin{figure}[h]
\begin{center}
\begin{algorithmic}
{\bf Algorithm} PerformManyIncrements($tl$): \\
\STATE $t$ = $tl$ * {CONSTANT} \\
\FOR{$k$ = 1 to $t$ by 1}
	\FOR{$i$ = 1 to {UINT\_MAX}-1 by 1}
		\STATE $j$ += 1 \\
	\ENDFOR 
\ENDFOR 
\end{algorithmic}
\end{center}
\caption{Computation by a Program-Under-Test (PUT)\label{alg:put}}
\vspace{-.2in}
\end{figure}

We used 128~seconds as the task length and ran the PUT program (termed PUT128) 
800~times. {\color{red}Note that PUT128 was run on a machine, with 
a specification illustrated in Table~\ref{tab:machine_config}, 
in the same timing environment as used in our prior work~\cite{Currim}, 
with i)~non-critical daemons deactivated, 
ii)~the Network Timing Protocol daemon switched on,  
iii)~particular CPU features\cite{intel15,intelSpeed15} turned off, and 
and iv)~an up-to-date kernel installed.} 

{\color{blue}Fig.~\ref{fig:meas_comp} shows two plots of the timing results on PUT128.
Most of the 800 executions measured in elapsed time, 
via  {\tt System.currentTimeMillis()} in Java, form the central band 
of \hbox{128,000--129,000 ms,} as can be seen in Fig.~\ref{fig:hist_before_et}. 
Despite the well-controlled timing environment, there exist over a dozen of executions above that band, or {\em outliers}. Indeed, these outliers {\em critically} impact the overall measurement quality: a standard deviation of 1,730 ms and a relative error of 0.01. This result suggests adequately eliminating these outliers while retaining most of the executions.}

{\color{blue}
Our protocol, which we characterize shortly, on the contrary, 
exactly identified and removed those outliers. 
In effect, the protocol considerably reduced such variability, 
as illustrated in Fig.~\ref{fig:hist_after_pt}. 
Specifically, regarding standard deviation and relative error 
the protocol presented a significant performance gain up to by 
three orders of magnitude for the same PUT128 results.
}

{\color{red}
{\bf Characteristics.} There are several distinctions of our protocol. 
First, we utilize {\em process time} (PT) rather than on elapsed time (ET). 
The use of PT is preferred, as it takes into account 
the time taken for only a ``process'' (or program) of interest.
PT is defined as the sum of ticks (where one tick is equal to 10 msec)
in user and system mode. 
These tick measures are obtainable via {\tt taskstats} C struct, 
provided by the Linux NetLink facility~\cite{Netlink}. 
Second, the protocol throws out some executions that involve
{\em infrequent, \hbox{long-running} daemon processes}. 
We observe that such daemon processes substantially impact 
the execution time of the process.  
By eliminating those executions, the protocol greatly improves
the overall measurement quality, which would otherwise be biased by them.
}

{\bf Related Work.} 
McGeoch introduced
two basic methods of measuring program time: elapsed time and CPU time~\cite{Mcgeoch12}. 
Bryant and O'Hallaron~\cite{Randal03} 
presented two timing schemes of using clock-cycle and interval counters. 
They proposed a measurement protocol, called {\em minimum-of-k}, 
that for observed elapsed ticks the minimum is chosen as the most accurate one. 
Odom et al.'s work~\cite{Odom05} focused on timing long-running programs 
in a simulation framework via dynamic sampling of trace snippets during program execution. 
But none of these prior works considers the variability in 
timing and the impact of daemons that may disturb the timing a lot.

\begin{figure}[t]
	\centering
	\subfigure[{\color{blue}Initial Elapsed Time Measurement}]{
		\includegraphics[width=0.224\textwidth]{128_sec_et_old.eps}
		\label{fig:hist_before_et}
	}
	\subfigure[{\color{blue}Program Time After Our Protocol}]{
		\includegraphics[width=0.224\textwidth]{128_sec_pt_new.eps}
		\label{fig:hist_after_pt}
	}
	\caption{{\color{blue}An Example of Measurement Results on a \linebreak \hbox{Compute-bound}} 
	Program-Under-Test~\label{fig:meas_comp}}
	\vspace{-.18in}
\end{figure} 

Commercial software tools measure execution time~\cite{VTune,TimeSys,WindView}. 
Since the tools' source code is not disclosed, there is no way of figuring out 
whether they can prevent such a daemon from timing. 

\vspace{0.05in}
{\bf Contribution.} 
Our contributions are following.
\vspace{-0.05in}
\begin{itemize}

\item We show empirical evidence that 
measuring execution time 
can be seriously affected by daemons.

\item We propose a novel timing protocol that
identifies infrequent, long-running daemons that impact the timing results for that program. 

\item {\color{blue}We evaluate the performance of the protocol with rigorous experiments 
using real-world programs.}

\item The experimental results show a support for the effectiveness of our scheme\shorten{SEDONA: 
early eliminating dirty executions infected with such a daemon 
and eventually contributing to enhancing the overall measurement-quality}. 

\end{itemize}
\vspace{-0.05in}

\noindent
The rest of this letter is organized as follows. 
{\color{blue}We next elaborate on the proposed timing protocol. 
In turn, we evaluate the performance of the scheme using real workloads. 
Finally, we conclude this letter.}

%\vspace\fill

\section{Proposed Scheme}
\label{sec:prop_appach}

In this section we propose a novel execution time measurement scheme, 
called {\em SEDONA} (Selective Elimination through Detection of infrequent, lOng-ruNning dAemons). {\color{blue} Our scheme (1) utilizes PT (process time) to avoid absorbing timing noises from other co-running processes and (2) catches and eliminates executions including daemon processes that are infrequent and \hbox{long-running} via a {\em cutoff} measure. 
%These two features contribute to significantly enhancing measurement quality.
}

Our SEDONA protocol consists of a total of ten steps, as described in Fig.~\ref{alg:find}. 
{\color{red}Note that this protocol can be applied to any arbitrary Linux system.}

\begin{figure}[h]
\begin{center}
\begin{algorithmic}
{\bf Algorithm} The SEDONA Timing Protocol: \\
\STATE Step 1. Set up the timing environment.
\STATE Step 2. Perform a single PUT run (specifically, PUT128) for many samples (specifically, 800).
\STATE Step 3. Consider each pair of elapsed time measurements to be a dual-PUT measurement 
and examine a scatter-plot to see if it it displays an $L$-shape.
\STATE Step 4. Zoom into the central cluster to ensure that it is symmetric (roughly circular).
\STATE Step 5. Compute the maximum and standard deviation of the process time 
for each daemon encountered within the central cluster samples.
\STATE Step 6. Identify for each sample in the $L$-shape infrequent, long-running daemon executions. 
\STATE Step 7. Determine potentially periodic daemons based on the $L$-executions 
and for each daemon compute the minimum process time from those executions identified. 
\STATE Step 8. Perform Steps 1--6 above for a single run 
consisting of a small number of executions (specifically, 40) of PUT16384.  
\STATE Step 9. Compute the cutoffs for each identified daemon. 
\STATE Step 10. Discard an execution including a daemon of which process time is greater than the respective cutoff time.
\end{algorithmic}
\end{center}
\caption{Summary of the SEDONA Timing Protocol\label{alg:find}}
\vspace{-0.25in}
\end{figure}

{\bf A Running Example.} 
%As motivated by our prior work~\cite{Currim}, 
%we configure a timing environment by i) stopping non-critical daemons, 
%ii) activating the Network Timing Protocol daemon, and 
%iii) switching off particular CPU features\cite{intel15,intelSpeed15} if any (Step 1). 
{\color{blue}
First, we set up the same measurement environment as described earlier in Section~\ref{sec:intro}, 
to eliminate known timing factors in advance (Step~1).}

We then run a program-under-test (called PUT) of 
Fig.~\ref{alg:put} with a task length of 128~sec, 
termed {\em PUT128}, 800 times (Step 2). 
(We render Fig.~\ref{fig:hist_before_et} using the 800 samples from this run.)
We use 128 seconds because that is long enough to perhaps experience an infrequent daemon. 
We run it 800 times to capture infrequent daemons that perhaps run every few hours or even 
once a day. 
Note that we collect all daemon processes as well as the PUT and their measures 
through the Netlink interface from the kernel before and after each 
timing.

Fig.~\ref{fig:reg_put128} plots all the 800 elapsed times (ETs) of the run of PUT128.
%We see three rows in the plot: a solid row of many samples, perhaps
%six or more samples that are just above that solid row, and two
%samples that are way above the solid row. 
The plot clearly shows three rows; that is, 
the top and middle rows represent over a dozen of outliers far from 
the rest of the samples clustered in the bottom row. 
We'll now drill down into these outliers to 
show how to reliably eliminate the indirect influence of 
some ``infrequent, long-running daemons'' on PT (\hbox{process} time) of the PUT.

To identify such daemons, we use a novel \hbox{scatter} plot: 
those of {\em pairs of successive samples}{\color{red}. 
So \hbox{samples}~\#{1} and~\#{2} form the first pair, 
samples~\#{3} and~\#{4} form the second pair.
Such a pair is termed ``\hbox{{\it dual-PUT}}.'' 
A sample of \hbox{dual-PUT256}, for instance, 
consists of two consecutive (odd and even) 
samples of PUT128: 
sample~\#{1} of dual-PUT256 is equivalent 
to samples~\#{1} and \#{2} of PUT128.}

Fig.~\ref{fig:raw_put128} presents such a scatter plot 
of 400 samples of a run of \hbox{dual-PUT256} constructed 
from a run of the 800 PUT128 samples (Step~3). 
{\color{red} The $xy$-plane corresponds to the dual-PUT samples; 
for example, \hbox{sample}~\#{2} of a run of dual-PUT256 
is plotted as a point with samples \#{3} and~\#{4} of a run of PUT128 
on the respective $x$ and $y$ axes. In that sense 
the $x$ ($y$) axis is named as ``Odd (Even) Sample's ET.''}
There are two quite obvious outliers with ETs 
of 163,913 ms (rightmost) and 161,785 ms (uppermost), respectively.

\begin{figure}[ht]
	\centering
	\subfigure[PUT128 with 800 Samples]{
		%\includegraphics[width=0.223\textwidth]{put128_empv4.eps}
		\includegraphics[width=0.224\textwidth]{put128_empv4_new.eps}
		\label{fig:reg_put128}
	}
	\subfigure[Dual-PUT256 Samples]{
		%\includegraphics[width=0.223\textwidth]{dual_et_put256_empv4.eps}
		\includegraphics[width=0.224\textwidth]{dual_et_put256_empv4.eps}
		\label{fig:raw_put128}
	}
	%\vspace{-0.1in}    
	\subfigure[Zooming in on the $L$-shape]{
		\includegraphics[width=0.224\textwidth]{zooming_in_dual_et_put256_empv4.eps}
		\label{fig:dual_put256_liters}
	}
	\subfigure[The Central Cluster]{
		\includegraphics[width=0.224\textwidth]{dual_et_clump_put256_empv4.eps}
		\label{fig:dual_et_put256_cluster}
        }
    \caption{Successive Scatter Plots of PUT128 with 800 samples 
    (equivalent to Dual-PUT256 with 400 samples) in Steps~2---4~\label{fig:put128_plot}} 
    \vspace{-0.3in}     
\end{figure} 

We informally term this phenomenon of a scatter plot of a dual-PUT run an
``$L$-shape,'' and attribute it to the
presence of infrequent long-running daemons.\shorten{Again, samples containing such
daemons (termed the $L$-samples) will appear
along the left y-axis or along the bottom x-axis, but will not occur in the
top right of the scatter plot of the dual-PUT.}

Figure~\ref{fig:dual_put256_liters} zooms into the lower left region,
focusing on the
tight cluster of samples.  Interestingly, this plot continues to exhibit an
$L$-shape, with perhaps a dozen or more $L$-samples in the left and bottom arms of
the ``$L$'', and again no samples in the upper right portion of the
scatter plot.

We continue zooming until we get to Figure~\ref{fig:dual_et_put256_cluster},
which shows a central cluster (Step 4). 
We confirm the symmetry of the ET measurements in the central cluster: there
is no $L$-shape, and thus no $L$-samples, and thus no obvious infrequent
long-running daemons.\shorten{(We emphasize that these samples may have lots of
frequent daemons, as well as infrequent, {\em short-running} daemons.) There
were 384 dual-PUT256 samples in this central cluster, implying that 16 of
the PUT128 samples were $L$-samples and the 785 remaining of the original 800 PUT128
samples had no infrequent, long-running daemons, which motivates that term.}

We then perform Step~5, which computes 
the maximum process time and standard deviation 
of PT (process time, note the switch in emphasis from elapsed time 
to process time) of the daemon processes 
(e.g. {\tt cifsd}, {\tt flush-9:0}, {\tt jbd2/md0-8}, 
{\tt kblockd/0}, {\tt khugepaged}, {\tt md0\_raid1}, and {\tt ntpd})
observed in the central cluster samples in Fig.~\ref{fig:dual_et_put256_cluster}. 
\shorten{In the running example the maximum PT of those daemons was under 4 msec, 
and their standard deviation was just about 1 msec.}

In Step~6 we identify, for each daemon in the \hbox{$L$-samples}, 
those that are actual long-running daemon
executions.  We define such executions as those whose PT 
is over two standard deviations above the maximum PT for
that daemon in the central cluster samples. 
In the running example  {\tt flush-9:0}, 
{\tt jbd2/md0-8}, and {\tt md0\_raid1} 
are determined as infrequent, long-running.
We also identify ``extra'' infrequent daemons: those
found only in the $L$-samples but not in the central cluster. 
The running example reveals the following extra daemons: 
{\tt bash}, {\tt grep}, {\tt rhn\_check}, {\tt rhnsd}, {\tt rhsmcertd}, 
{\tt rhsmcertd-worke}, and {\tt sshd}.

For each of the infrequent daemons 
we use a heuristic to determine the daemon's periodicity: 
the daemon must occur regularly in a sequence of samples.
For instance, the {\tt rhn\_check} daemon appears 
roughly every 112 samples (which corresponds to very close to every four 
hours). Four others ({\tt flush-9:0}, {\tt jbd2/md0-8}, 
{\tt md0\_raid1}, and {\tt rhn\_check}) all occur together 
(in those two outliers in Fig.~\ref{fig:raw_put128}) and 
  have a periodicity of about every 559 samples
  (5x longer, or just about 20 hours). 
%  It is important only which samples a given daemon shows up 
%  in; we don't care in this heuristic whether daemons are co-occurring.

Next, we can compute for each so-identified infrequent, long-running daemon its
minimum time in the \hbox{$L$-samples} (Step 7). 
This computation provides a rough, initial distinction of a ``long-running''
%daemon, namely, the valley between the maximum PT from the central cluster 
daemon, or the valley between the maximum PT from the central cluster 
and the minimum PT from the $L$-samples, to differentiate ``short-running'' 
from ``\hbox{long-running}'' executions of the daemon. 
For those daemons (i.e. {\tt grep}) never appearing in the central cluster,
this initial analysis concludes only that they are infrequent.

In Step 8 we repeat Steps 1--6, but instead with the much-longer running
PUT16384 (4.5 hours per \hbox{sample} versus 2 minutes), to see 
if any of our identified \hbox{infrequent} daemons are actually frequent 
at that much longer PUT execution time. 
We find some frequent daemon processes appearing in both of the 
clusters of dual-PUT256 and dual-PUT32768{\color{blue}, each consisting of 
pairs of two successive samples of PUT128 and PUT16384}: 
{\tt flush-9.0}, {\tt jbd2/md0-8}, {\tt kblock/0}, {\tt md0\_raid1}, 
and {\tt ntpd}. That said, the central cluster also contains other
  processes not seen in the \hbox{dual-PUT256} central cluster: 
  {\tt grep}, {\tt rhn\_check}, {\tt rhnsd}, {\tt rhsmcertd}, 
  {\tt rhsmcertd-worke}, and {\tt sshd}. 
  But these daemons were categorized in the \hbox{dual-PUT256} analysis as {\em
  infrequent}, several having \hbox{periodicities} estimated at four or twenty hours.
When {\color{blue}PUT128} had a ``short'' 
program time (or, two minutes), daemons with a periodicity of
hours are \hbox{infrequent}. But with {\color{blue}PUT16384} 
with a ``long'' program time (or, 4.5 hours), some of those daemons are now frequent, and appear in the central cluster.

In Step~9 we compute 
the cutoff for each of those infrequent, long-running daemons so
identified, based on the runs of PUT128 and the 
PUT16384 as collected in Tab.~\ref{tab:final_infrequent_cutoff}. 
Here is how to compute the cutoff. For the cutoff of such a daemon with PUT128, 
we take the \hbox{midpoint} between the maximum of that daemon's PTs in the 
central cluster (or 0, if absent) and the \hbox{minimum} of those in the $L$-samples. 
For the cutoff of such a daemon with PUT16384, we do the same. 
We then compute a ``task time'' as 5\% of the inferred periodicity. 
This 5\% ensures that such infrequent daemons will impact 
only a small percentage of the shorter PUTs, while presumably being 
associated with much larger cutoffs for the very long PUTs. 
We also include daemons that (a)~were identified as infrequent and 
\hbox{long-running} from PUT128 and (b)~were not identified as so in the PUT16384 
\hbox{$L$-samples}, but may have in the \hbox{dual-PUT32768} central cluster.
We then take the {\em maximum} of the two cutoffs for the final cutoff PT 
(the last column of Tab.~\ref{tab:final_infrequent_cutoff}). 

\begin{table}[ht]
\centering
{\scriptsize
\begin{tabular}{|p{1.2cm}|c|c|p{1cm}|p{1.45cm}|} \hline
{\tiny Process}  & {\tiny Cutoff PT} & {\tiny Cutoff PT} & {\tiny Task}  & {\tiny{\bf Final }} \\
{\tiny Name} & {\tiny on PUT128}  & {\tiny on PUT16K} & {\tiny Time} & {\tiny {\bf Cutoff PT}} \\\hline
%{\tiny Process}  & {\tiny Cutoff PT} & {\tiny Cutoff PT} & {\tiny Task}  & {\tiny{\bf Final }} \\
% Name & on PUT128  & on PUT16K & Time & {\bf Cutoff PT} \\\hline
{\tt bash} & 1 msec & --- & --- & {\bf 1 msec} \\ \hline
{\tt flush-9:0} & 64 msec & --- & $< 1$ hour & {\bf 64 msec} \\
                & ---     & 48 msec & $\geq 1$ hour & {\bf 48 msec} \\ \hline
{\tt grep }     & 1 msec & 12 msec & --- & {\bf 12 msec}\\ \hline
{\tt jbd2/md0-8} & 4 msec & --- & $< 1$ hour & {\bf 4 msec} \\
                & ---     & 11 msec & $\geq 1$ hour & {\bf 11 msec} \\ \hline
{\tt md0\_raid1} & 35 msec & ---     & $< 1$ hour  & {\bf 35 msec}\\
                 & ---     & 51 msec & $\geq 1$ hour & {\bf 51 msec} \\ \hline
{\tt rhn\_check}  & 281 msec & --- & $< 12$ min & {\bf 281 msec} \\
                 &  --- & 12,828 msec & $\geq 12$ min & {\bf 12,828 msec}\\ \hline
{\tt rhnsd} & 2 msec & --- & $< 12$ min & {\bf 2 msec} \\
            & ---    & 12 msec &$\geq 12$ min & {\bf 12 msec} \\ \hline
{\tt rhsmcertd}  & 1 msec & 1 msec & --- & {\bf 1 msec} \\  \hline
{\tt rhsmcertd}  & 57 msec & --- & $< 12$ min & {\bf 57 msec} \\
{\tt -worke}           &  --- & 119 msec  & $\geq 12$ min & {\bf 119 msec}\\ \hline
{\tt sshd} & 2 msec & 23 msec & --- & {\bf 23 msec}\\ \hline
\end{tabular}
}
\caption{Collected Infrequent, Long-running Daemons and Their Final Cutoff
  Process Time \hbox{(Step~9)}\label{tab:final_infrequent_cutoff}}
  \vspace{-0.4in}
\end{table}

Based on Tab.~\ref{tab:final_infrequent_cutoff}, 
we discard any \hbox{sample} \hbox{containing} an infrequent, long-running daemon execution
over the corresponding cutoff. We thus end up dropping just fifteen of the 800
PUT128 samples and only two of the forty PUT16384 samples. 
As a result, the \hbox{overall} \hbox{measurement} quality---the standard deviation and \hbox{relative} error 
---for PUT128 was improved 
by three orders of magnitude (as shown in Fig.~\ref{fig:meas_comp}) 
and by about two \hbox{orders} of magnitude for PUT16384.
%(5661.7 vs. 229.8) 0.0003448491 vs. 8.4$\times$10$^{-6}$
%(40.4 vs. 26.7) 2.463122e-06 vs. 1.626005e-06

%\vspace\fill

\section{Evaluation}
\label{sec:eval}
\vspace{-0.07in}
We now evaluate the \hbox{performance} of SEDONA 
compared to that of the conventional timing scheme, termed ORG, based on 
elapsed time.
Our experiments were conducted on a \hbox{machine}
described in Table~\ref{tab:machine_config}. 
\begin{table}[h]
\vspace{-0.2in}
\begin{center}
{\scriptsize
\begin{tabular}{|l|p{7cm}|}\hline
OS & Red Hat Ent. Linux (RHEL) 6.4 with a kernel of 2.6.32 \\ \hline
CPU & Intel Core i7-870 Lynnfield 2.93GHz quad-core \hbox{processor}\shorten{ on a LGA 1156 95W motherboard}\\ \hline
RAM & 4GB of DDR3 1333 dual-channel memory\\ \hline
HDD & Western Digital Caviar Black 1TB 7200rpm SATA Drive\\ \hline
\end{tabular}
}
\end{center}
\caption{Machine Configurations\label{tab:machine_config}}
\vspace{-0.3in}
\end{table}

{\color{blue}
First, we compared the performance of the two approaches 
using real-world programs such as insertion sort, termed SORT and 
matrix multiplication (in column major), termed MM. 
In this comparison SEDONA overwhelmed ORG, as can be seen in Fig.~\ref{fig:synprog_test}. 
As workload size increased, the performance gap between the two 
got increasingly widened, reaching up to by 6x.
}

In turn, our evaluation proceeded with SPEC CPU2006 
benchmarks~\cite{specCpu2006}, providing various \linebreak 
\hbox{compute-bound} real applications. The results are \hbox{provided} in Table~\ref{tab:spec_real}. 
Note that in the table the results for {\tt 481} and {\tt 483} \hbox{benchmarks} 
are omitted because of some runtime error and incurred I/O, respectively.

{\color{blue}
Table~\ref{tab:spec_real} shows that 
SEDONA outperformed ORG,
on the standard deviation and relative error across the very different SPEC benchmarks. 
To be more specific, all the benchmarks revealed a smaller standard deviation 
from SEDONA as compared to that of ORG. 
Our timing protocol quite effectively filtered out infrequent daemon executions 
in the industrial workloads. The relative error of SEDONA was also 
lower than that of ORG for almost every benchmark.
{\color{red}About a 10x \hbox{margin} between the two was yielded for {\tt 434}, for example.}
}

\begin{figure}[t]
	\centering
	\subfigure[{\color{blue}SORT - Standard Deviation}]{
		\includegraphics[scale=0.31]{sort_std.eps}
		\label{fig:sort_std}
	}
	\subfigure[{\color{blue}SORT - Relative Error}]{
		\includegraphics[scale=0.31]{sort_re.eps}
		\label{fig:sort_re}
	}
	\subfigure[{\color{blue}MM - Standard Deviation}]{
		\includegraphics[scale=0.31]{matc_std.eps}
		\label{fig:matc_std}
	}
	\subfigure[{\color{blue}MM - Relative Error}]{
		\includegraphics[scale=0.31]{matc_re.eps}
		\label{fig:matc_re}
	}
	\vspace{-.1in}
	\caption{{\color{blue}Performance Comparison on Real-world Programs~\label{fig:synprog_test}}}
\vspace{-.2in}
\end{figure}

%\vspace{-.15in}
SEDONA also scaled well for the SPEC workloads, 
with regard to growth of relative error as the execution time lengthened.
For the short benchmarks 
(e.g., {\tt 400}, {\tt 403}, {\tt 410}, 
{\tt 434}, {\tt 445}, and {\tt 999}: those taking \hbox{under} 100 sec), 
our scheme outperformed the ORG method by about 3.5x, on average. 
The SEDONA protocol continued its dominance against the conventional technique 
for the medium-length benchmarks (e.g., {\tt 447}, {\tt 456}, {\tt 470}, and {\tt 473}).
{\color{blue}Even for the long-running benchmarks (e.g., {\tt 436}, {\tt 416}, and {\tt 454}, both$>$ 1,000 sec), 
the relative error of \hbox{SEDONA} was still lower than that of ORG.}

To summarize, our proposed SEDONA protocol can achieve {\em better} accuracy,
precision, and scalability in measuring the execution time of real \hbox{compute-bound} benchmarks than the conventional timing method. 
{\color{blue}The experimental results demonstrate 
the general applicability of SEDONA to timing any CPU-bound program.}

\vspace{-0.1in}
\begin{table}[t]
\centering
{
\scriptsize
\begin{tabular}{|p{0.18cm}|R{1.14cm}|R{1.14cm}|R{0.64cm}|R{1cm}|R{0.97cm}|R{0.97cm}|} \hline
 		  & \multicolumn{2}{c|}{{\tiny Execution Time (ms)}} & \multicolumn{2}{c|}{{\tiny Standard}}   & \multicolumn{2}{c|}{\multirow{2}{*}{{\tiny Relative Error}}}\\ \cline{2-3}
 & \multicolumn{1}{c|}{{\tiny{ORG}}}  & \multicolumn{1}{c|}{{\tiny{SEDONA}}} & \multicolumn{2}{c|}{{\tiny Deviation (ms)}}  & \multicolumn{2}{c|}{}\\ \cline{4-7}	
 & \multicolumn{1}{c|}{{\tiny{(in ET)}}} & \multicolumn{1}{c|}{{\tiny{(in PT)}}} & {\tiny{ORG}} & {\tiny{SEDONA}} & \multicolumn{1}{c|}{{\tiny{ORG}}}& \multicolumn{1}{c|}{{\tiny{SEDONA}}}\\ \hline
{{\tt 400}} & 454 & 445 & {3} & {2} & {6$\times$10$^{-3}$} & {3$\times$10$^{-3}$}\\
{{\tt 401}} & 536,639 & 528,517 & {1,185} & {1,161} & {2$\times$10$^{-3}$} & {2$\times$10$^{-3}$}\\
{{\tt 403}} & 26,109	& 25,695 & {138} & {96} & {5$\times$10$^{-3}$} & {4$\times$10$^{-3}$}\\
{{\tt 410}} & 7,938 & 7,801 & {46} & {19} & {6$\times$10$^{-3}$} & {2$\times$10$^{-3}$}\\
{{\tt 416}} & 1,015,342 & 999,846 & {965} & {876} & {1$\times$10$^{-3}$} & {9$\times$10$^{-4}$}\\%% V
{{\tt 429}} & 235,811 & 232,209 & {623} & {600} & {3$\times$10$^{-3}$} & {3$\times$10$^{-3}$}\\
{{\tt 433}} & 480,586 & 473,256  & {743} & {725} & {2$\times$10$^{-3}$} & {2$\times$10$^{-3}$}\\ %% V
{{\tt 434}} & 16,495  & 16,242  & {\color{red}{75}} & {\color{red}{7}} & {\color{red}{5$\times$10$^{-3}$}} & {\color{red}{5$\times$10$^{-4}$}}\\
{{\tt 435}} & 990,575 & 975,445  & {947} & {900} & {1$\times$10$^{-3}$} & {9$\times$10$^{-4}$}\\
{{\tt 436}} & 1,160,742 & 1,143,078   & {3,914} & {3,843} & {3$\times$10$^{-3}$} & {3$\times$10$^{-3}$}\\
{{\tt 437}} & 581,635 & 572,775 & {1,492} & {1,475}  & {3$\times$10$^{-3}$} & {3$\times$10$^{-3}$}\\
{{\tt 444}} & 591,201 & 582,229 & {294} & {281}  & {5$\times$10$^{-4}$} & {5$\times$10$^{-4}$}\\
{{\tt 445}} & 84,435  & 83,164  & {91} & {28}  & {1$\times$10$^{-3}$} & {3$\times$10$^{-4}$}\\
{{\tt 447}} & 521,846 & 513,493  & {150} & {108}  & {3$\times$10$^{-4}$} & {2$\times$10$^{-4}$}\\
{{\tt 450}} & 341,030 & 335,848  & {99} & {91}  & {3$\times$10$^{-4}$} & {2$\times$10$^{-4}$}\\
{{\tt 453}} & 258,797 & 254,496  & {623} & {582}  & {2$\times$10$^{-3}$} & {2$\times$10$^{-3}$}\\
{{\tt 454}} & 1,721,804 & 1,695,613  & {678} & {627}  & {4$\times$10$^{-4}$} & {4$\times$10$^{-4}$}\\
{{\tt 456}} & 410,533 & 404,328  & {85} & {50}  & {2$\times$10$^{-4}$} & {1$\times$10$^{-4}$}\\
{{\tt 458}} & 589,541 & 580,591  & {542} & {513} &  {9$\times$10$^{-4}$} & {9$\times$10$^{-4}$}\\
{{\tt 459}} & 798,917 & 786,726  & {2,143} & {2,132}  & {3$\times$10$^{-3}$} & {3$\times$10$^{-3}$}\\
{{\tt 462}} & 595,188 & 586,120  & {3,326} & {3,274}  & {6$\times$10$^{-3}$} & {6$\times$10$^{-3}$}\\
{{\tt 464}} & 649,838 & 639,939  & {601} & {563}  & {9$\times$10$^{-4}$} & {9$\times$10$^{-4}$}\\
{{\tt 465}} & 895,754 & 882,106  & {883} & {797}  & {1$\times$10$^{-3}$} & {1$\times$10$^{-3}$}\\
{{\tt 470}}	& 349,830 & 344,510 & {143} & {94} & {4$\times$10$^{-4}$} & {3$\times$10$^{-4}$}\\
{{\tt 471}} & 367,589 & 361,959  & {2,114} & {2,072} & {6$\times$10$^{-4}$} & {6$\times$10$^{-4}$}\\
{{\tt 473}} & 362,587 & 357,090  & {359} & {317} &  {1$\times$10$^{-3}$} & {9$\times$10$^{-4}$}\\
%{{\tt 481}} & \multicolumn{4}{c}{Runtime Error}\\
{{\tt 482}} & 654,208	 & 644,223  & {2,436} & {2,392} &   {4$\times$10$^{-3}$} & {4$\times$10$^{-3}$}\\ %% V
%{{\tt 483}} & \multicolumn{4}{c}{Out-of-Scope}\\
{{\tt 998}} & 128	 & 127  & {0.6} & {0.6} &  {4$\times$10$^{-3}$} & {4$\times$10$^{-3}$}\\
{{\tt 999}} & 128 &  127 & {0.8} & {0.6} &  {6$\times$10$^{-3}$} & {5$\times$10$^{-3}$}\\ \hline 
\multicolumn{3}{|r|}{Averages} & 852 & 815 &  {3$\times$10$^{-3}$} & {2$\times$10$^{-3}$}\\ \hline
\end{tabular}
}
\caption{Performance Evaluation on the SPEC Benchmarks\label{tab:spec_real}}
\vspace{-0.25in}
\end{table}

{\color{red}
\section{Conclusion}
\label{sec:conclusion}
\vspace{-0.07in}
We \shorten{provided an empirical evidence of 
how significantly measurement quality could be hurt by 
the presence of infrequent, long-running daemon processes. 
We }proposed a novel timing protocol called {\em SEDONA}\shorten{, 
employing process time and identifying an infrequent, long-running daemon}, 
more precise and \hbox{accurate} than the traditional method, 
and performed its rigorous empirical evaluation\shorten{
as shown in the experiments}.
}

\balance

\vspace{-0.03in}
{\tiny 
\bibliographystyle{ieicetr} 
\bibliography{paper}
%\vspace{-0.2in}
}

\vspace\fill
\end{document}
